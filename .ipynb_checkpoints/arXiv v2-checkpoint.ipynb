{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c08a9e1-c017-4956-8761-4f4b1de0c596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# резервный список категорий на случай если парсинг сайта не сработает\n",
    "TAXONOMY_R = {'math.AC': {'name': 'Commutative Algebra',\n",
    "  'description': 'Commutative rings, modules, ideals, homological algebra, computational aspects, invariant theory, connections to algebraic geometry and combinatorics'},\n",
    " 'math.AG': {'name': 'Algebraic Geometry',\n",
    "  'description': 'Algebraic varieties, stacks, sheaves, schemes, moduli spaces, complex geometry, quantum cohomology'},\n",
    " 'math.AP': {'name': 'Analysis of PDEs',\n",
    "  'description': \"Existence and uniqueness, boundary conditions, linear and non-linear operators, stability, soliton theory, integrable PDE's, conservation laws, qualitative dynamics\"},\n",
    " 'math.AT': {'name': 'Algebraic Topology',\n",
    "  'description': 'Homotopy theory, homological algebra, algebraic treatments of manifolds'},\n",
    " 'math.CA': {'name': 'Classical Analysis and ODEs',\n",
    "  'description': \"Special functions, orthogonal polynomials, harmonic analysis, ODE's, differential relations, calculus of variations, approximations, expansions, asymptotics\"},\n",
    " 'math.CO': {'name': 'Combinatorics',\n",
    "  'description': 'Discrete mathematics, graph theory, enumeration, combinatorial optimization, Ramsey theory, combinatorial game theory'},\n",
    " 'math.CT': {'name': 'Category Theory',\n",
    "  'description': 'Enriched categories, topoi, abelian categories, monoidal categories, homological algebra'},\n",
    " 'math.CV': {'name': 'Complex Variables',\n",
    "  'description': 'Holomorphic functions, automorphic group actions and forms, pseudoconvexity, complex geometry, analytic spaces, analytic sheaves'},\n",
    " 'math.DG': {'name': 'Differential Geometry',\n",
    "  'description': 'Complex, contact, Riemannian, pseudo-Riemannian and Finsler geometry, relativity, gauge theory, global analysis'},\n",
    " 'math.DS': {'name': 'Dynamical Systems',\n",
    "  'description': 'Dynamics of differential equations and flows, mechanics, classical few-body problems, iterations, complex dynamics, delayed differential equations'},\n",
    " 'math.FA': {'name': 'Functional Analysis',\n",
    "  'description': 'Banach spaces, function spaces, real functions, integral transforms, theory of distributions, measure theory'},\n",
    " 'math.GM': {'name': 'General Mathematics',\n",
    "  'description': 'Mathematical material of general interest, topics not covered elsewhere'},\n",
    " 'math.GN': {'name': 'General Topology',\n",
    "  'description': 'Continuum theory, point-set topology, spaces with algebraic structure, foundations, dimension theory, local and global properties'},\n",
    " 'math.GR': {'name': 'Group Theory',\n",
    "  'description': 'Finite groups, topological groups, representation theory, cohomology, classification and structure'},\n",
    " 'math.GT': {'name': 'Geometric Topology',\n",
    "  'description': 'Manifolds, orbifolds, polyhedra, cell complexes, foliations, geometric structures'},\n",
    " 'math.HO': {'name': 'History and Overview',\n",
    "  'description': 'Biographies, philosophy of mathematics, mathematics education, recreational mathematics, communication of mathematics, ethics in mathematics'},\n",
    " 'math.IT': {'name': 'Information Theory',\n",
    "  'description': 'math.IT is an alias for cs.IT. Covers theoretical and experimental aspects of information theory and coding.'},\n",
    " 'math.KT': {'name': 'K-Theory and Homology',\n",
    "  'description': 'Algebraic and topological K-theory, relations with topology, commutative algebra, and operator algebras'},\n",
    " 'math.LO': {'name': 'Logic',\n",
    "  'description': 'Logic, set theory, point-set topology, formal mathematics'},\n",
    " 'math.MG': {'name': 'Metric Geometry',\n",
    "  'description': 'Euclidean, hyperbolic, discrete, convex, coarse geometry, comparisons in Riemannian geometry, symmetric spaces'},\n",
    " 'math.MP': {'name': 'Mathematical Physics',\n",
    "  'description': 'math.MP is an alias for math-ph. Articles in this category focus on areas of research that illustrate the application of mathematics to problems in physics, develop mathematical methods for such applications, or provide mathematically rigorous formulations of existing physical theories. Submissions to math-ph should be of interest to both physically oriented mathematicians and mathematically oriented physicists; submissions which are primarily of interest to theoretical physicists or to mathematicians should probably be directed to the respective physics/math categories'},\n",
    " 'math.NA': {'name': 'Numerical Analysis',\n",
    "  'description': 'Numerical algorithms for problems in analysis and algebra, scientific computation'},\n",
    " 'math.NT': {'name': 'Number Theory',\n",
    "  'description': 'Prime numbers, diophantine equations, analytic number theory, algebraic number theory, arithmetic geometry, Galois theory'},\n",
    " 'math.OA': {'name': 'Operator Algebras',\n",
    "  'description': 'Algebras of operators on Hilbert space, C^*-algebras, von Neumann algebras, non-commutative geometry'},\n",
    " 'math.OC': {'name': 'Optimization and Control',\n",
    "  'description': 'Operations research, linear programming, control theory, systems theory, optimal control, game theory'},\n",
    " 'math.PR': {'name': 'Probability',\n",
    "  'description': 'Theory and applications of probability and stochastic processes: e.g. central limit theorems, large deviations, stochastic differential equations, models from statistical mechanics, queuing theory'},\n",
    " 'math.QA': {'name': 'Quantum Algebra',\n",
    "  'description': 'Quantum groups, skein theories, operadic and diagrammatic algebra, quantum field theory'},\n",
    " 'math.RA': {'name': 'Rings and Algebras',\n",
    "  'description': 'Non-commutative rings and algebras, non-associative algebras, universal algebra and lattice theory, linear algebra, semigroups'},\n",
    " 'math.RT': {'name': 'Representation Theory',\n",
    "  'description': 'Linear representations of algebras and groups, Lie theory, associative algebras, multilinear algebra'},\n",
    " 'math.SG': {'name': 'Symplectic Geometry',\n",
    "  'description': 'Hamiltonian systems, symplectic flows, classical integrable systems'},\n",
    " 'math.SP': {'name': 'Spectral Theory',\n",
    "  'description': 'Schrodinger operators, operators on manifolds, general differential operators, numerical studies, integral operators, discrete models, resonances, non-self-adjoint operators, random operators/matrices'},\n",
    " 'math.ST': {'name': 'Statistics Theory',\n",
    "  'description': 'Applied, computational and theoretical statistics: e.g. statistical inference, regression, time series, multivariate analysis, data analysis, Markov chain Monte Carlo, design of experiments, case studies'}\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3b6c74f-d09c-4219-b772-f9c6f0800c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import os\n",
    "\n",
    "# ==========================================\n",
    "# КОНФИГУРАЦИЯ\n",
    "# ==========================================\n",
    "\n",
    "# Путь к твоему ключу (как в твоем исходном файле)\n",
    "KEY_PATH = \"/home/nkrishelie/Python/Store/AllsoftEcom.json\" \n",
    "PROJECT_ID = \"burnished-yeti-250015\"\n",
    "DATASET_ID = \"arXiv\"\n",
    "TABLE_ID = \"articles\"\n",
    "FULL_TABLE_REF = f\"{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}\"\n",
    "\n",
    "# Настройки парсинга\n",
    "DAYS_TO_FETCH = 1  # Скачиваем только за последнюю неделю (инкрементальная загрузка)\n",
    "ANALYSIS_PERIOD_DAYS = 365 # Анализируем данные за год\n",
    "TOP_LIMIT_PER_CAT = 15\n",
    "\n",
    "# Инициализация клиента BQ\n",
    "credentials = service_account.Credentials.from_service_account_file(KEY_PATH)\n",
    "bq_client = bigquery.Client(project=PROJECT_ID, credentials=credentials)\n",
    "arxiv_client = arxiv.Client()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f54ff51f-5fe0-4c03-9c74-3a4907292b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to fetch taxonomy from arXiv...\n",
      "Success: Fetched 146 categories (Cache had 0).\n",
      "Updating cache file...\n",
      "Final Taxonomy loaded: 146 categories.\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# ШАГ 1: УНИВЕРСАЛЬНАЯ ТАКСОНОМИЯ С КЭШИРОВАНИЕМ\n",
    "# ==========================================\n",
    "\n",
    "TAXONOMY_CACHE_FILE = \"arxiv_taxonomy_cache.json\"\n",
    "\n",
    "def get_universal_taxonomy():\n",
    "    \"\"\"\n",
    "    1. Парсит сайт arXiv.\n",
    "    2. Если успешно и данных много -> сохраняет в JSON.\n",
    "    3. Если неудачно или данных мало -> читает из JSON.\n",
    "    \"\"\"\n",
    "    url = \"https://arxiv.org/category_taxonomy\"\n",
    "    fetched_taxonomy = {}\n",
    "    \n",
    "    # --- 1. Попытка скачивания ---\n",
    "    print(\"Attempting to fetch taxonomy from arXiv...\")\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            sections = soup.find_all('h2', class_='accordion-head')\n",
    "            for section in sections:\n",
    "                section_name = section.get_text(strip=True)\n",
    "                \n",
    "                # Определяем группу\n",
    "                group_code = \"other\"\n",
    "                if \"Mathematics\" in section_name: group_code = \"math\"\n",
    "                elif \"Computer Science\" in section_name: group_code = \"cs\"\n",
    "                elif \"Physics\" in section_name: group_code = \"physics\"\n",
    "                elif \"Biology\" in section_name: group_code = \"bio\"\n",
    "                elif \"Finance\" in section_name: group_code = \"fin\"\n",
    "                elif \"Statistics\" in section_name: group_code = \"stat\"\n",
    "                elif \"Economics\" in section_name: group_code = \"econ\"\n",
    "                elif \"Electrical\" in section_name: group_code = \"eess\"\n",
    "\n",
    "                content_block = section.find_next_sibling('div', class_='accordion-body')\n",
    "                if content_block:\n",
    "                    for h4 in content_block.find_all('h4'):\n",
    "                        full_text = h4.get_text(separator=' ', strip=True)\n",
    "                        match = re.search(r'([a-z\\-]+\\.[A-Z\\-a-z]{2,})\\s*\\((.+)\\)', full_text)\n",
    "                        \n",
    "                        if match:\n",
    "                            code = match.group(1).strip()\n",
    "                            name = match.group(2).strip()\n",
    "                            \n",
    "                            desc = \"\"\n",
    "                            parent = h4.find_parent('div', class_='column')\n",
    "                            if parent:\n",
    "                                desc_col = parent.find_next_sibling('div', class_='column')\n",
    "                                if desc_col and desc_col.find('p'):\n",
    "                                    desc = desc_col.find('p').get_text(strip=True)\n",
    "                            \n",
    "                            fetched_taxonomy[code] = {\n",
    "                                'name': name, \n",
    "                                'description': desc,\n",
    "                                'group': group_code\n",
    "                            }\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching from web: {e}\")\n",
    "\n",
    "    # --- 2. Работа с Кэшем ---\n",
    "    cached_taxonomy = {}\n",
    "    if os.path.exists(TAXONOMY_CACHE_FILE):\n",
    "        try:\n",
    "            with open(TAXONOMY_CACHE_FILE, 'r', encoding='utf-8') as f:\n",
    "                cached_taxonomy = json.load(f)\n",
    "            print(f\"Found cached taxonomy with {len(cached_taxonomy)} entries.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading cache: {e}\")\n",
    "\n",
    "    # --- 3. Принятие решения (Validation & Fallback) ---\n",
    "    \n",
    "    # Если скачали меньше, чем было в кэше (или 0) — откатываемся на кэш\n",
    "    if len(fetched_taxonomy) < len(cached_taxonomy):\n",
    "        print(f\"Warning: Fetched only {len(fetched_taxonomy)} categories, but cache has {len(cached_taxonomy)}.\")\n",
    "        print(\"Using CACHED version to ensure data integrity.\")\n",
    "        final_taxonomy = cached_taxonomy\n",
    "        \n",
    "    elif len(fetched_taxonomy) > 0:\n",
    "        print(f\"Success: Fetched {len(fetched_taxonomy)} categories (Cache had {len(cached_taxonomy)}).\")\n",
    "        print(\"Updating cache file...\")\n",
    "        # Сохраняем новую версию\n",
    "        with open(TAXONOMY_CACHE_FILE, 'w', encoding='utf-8') as f:\n",
    "            json.dump(fetched_taxonomy, f, ensure_ascii=False, indent=2)\n",
    "        final_taxonomy = fetched_taxonomy\n",
    "        \n",
    "    else:\n",
    "        # И интернет не работает, и кэша нет\n",
    "        print(\"Critical Warning: Web fetch failed and no cache found.\")\n",
    "        final_taxonomy = {}\n",
    "\n",
    "    return final_taxonomy\n",
    "\n",
    "# Запуск\n",
    "TAXONOMY = get_universal_taxonomy()\n",
    "\n",
    "# --- 4. Хардкод на самый крайний случай (если и файла нет) ---\n",
    "if not TAXONOMY:\n",
    "    print(\"Using hardcoded fallback.\")\n",
    "    TAXONOMY = TAXONOMY_R\n",
    "\n",
    "print(f\"Final Taxonomy loaded: {len(TAXONOMY)} categories.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f32e20c1-1529-41bf-9aea-4e2f51f90271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching articles for the last 30 days...\n",
      "Uploading 2000 new articles to BigQuery...\n",
      "Temp table loaded.\n",
      "Merge complete.\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 2. EXTRACT & LOAD (ArXiv -> BigQuery)\n",
    "# ==========================================\n",
    "\n",
    "print(f\"Fetching articles for the last {DAYS_TO_FETCH} days...\")\n",
    "\n",
    "# Поиск по всем мат. категориям сразу\n",
    "search = arxiv.Search(\n",
    "    query = \"cat:math.*\",\n",
    "    max_results = 2000, # С запасом\n",
    "    sort_by = arxiv.SortCriterion.SubmittedDate\n",
    ")\n",
    "\n",
    "new_rows = []\n",
    "cutoff_date = datetime.now() - timedelta(days=DAYS_TO_FETCH)\n",
    "\n",
    "for r in arxiv_client.results(search):\n",
    "    pub_date = r.published.replace(tzinfo=None)\n",
    "    if pub_date < cutoff_date:\n",
    "        break # Дальше идут старые статьи, останавливаемся\n",
    "    \n",
    "    # Фильтруем категории (оставляем только те, что в таксономии)\n",
    "    cats = [c for c in r.categories if c in TAXONOMY]\n",
    "    if not cats: continue\n",
    "\n",
    "    new_rows.append({\n",
    "        \"id\": r.entry_id.split('/')[-1],\n",
    "        \"title\": r.title.replace('\\n', ' '),\n",
    "        \"abstract\": r.summary.replace('\\n', ' '),\n",
    "        \"authors\": [a.name for a in r.authors],\n",
    "        \"categories\": cats,\n",
    "        \"primary_category\": r.primary_category,\n",
    "        \"published_date\": pub_date.strftime(\"%Y-%m-%d\"),\n",
    "        \"updated_at\": datetime.now().isoformat(),\n",
    "        \"url\": r.entry_id\n",
    "    })\n",
    "\n",
    "if new_rows:\n",
    "    print(f\"Uploading {len(new_rows)} new articles to BigQuery...\")\n",
    "    \n",
    "    # Используем временную таблицу для безопасного MERGE\n",
    "    temp_table_id = f\"{PROJECT_ID}.{DATASET_ID}.temp_upload\"\n",
    "    \n",
    "    # ИСПРАВЛЕНИЕ: Указываем ПОЛНУЮ схему для временной таблицы, \n",
    "    # чтобы она совпадала со структурой данных\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        schema=[\n",
    "            bigquery.SchemaField(\"id\", \"STRING\"),\n",
    "            bigquery.SchemaField(\"title\", \"STRING\"),\n",
    "            bigquery.SchemaField(\"abstract\", \"STRING\"),\n",
    "            bigquery.SchemaField(\"authors\", \"STRING\", mode=\"REPEATED\"),\n",
    "            bigquery.SchemaField(\"categories\", \"STRING\", mode=\"REPEATED\"),\n",
    "            bigquery.SchemaField(\"primary_category\", \"STRING\"),\n",
    "            bigquery.SchemaField(\"published_date\", \"DATE\"),\n",
    "            bigquery.SchemaField(\"updated_at\", \"TIMESTAMP\"),\n",
    "            bigquery.SchemaField(\"url\", \"STRING\"),\n",
    "        ],\n",
    "        write_disposition=\"WRITE_TRUNCATE\", # Перезаписываем временную таблицу при каждом запуске\n",
    "        source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON\n",
    "    )\n",
    "    \n",
    "    # Загружаем данные\n",
    "    try:\n",
    "        job = bq_client.load_table_from_json(new_rows, temp_table_id, job_config=job_config)\n",
    "        job.result() # Ждем завершения\n",
    "        print(\"Temp table loaded.\")\n",
    "\n",
    "        # MERGE запрос: обновляет старые, вставляет новые\n",
    "        # (Остается без изменений)\n",
    "        merge_query = f\"\"\"\n",
    "        MERGE `{FULL_TABLE_REF}` T\n",
    "        USING `{temp_table_id}` S\n",
    "        ON T.id = S.id\n",
    "        WHEN MATCHED THEN\n",
    "          UPDATE SET \n",
    "            T.title = S.title,\n",
    "            T.categories = S.categories,\n",
    "            T.updated_at = S.updated_at\n",
    "        WHEN NOT MATCHED THEN\n",
    "          INSERT (id, title, abstract, authors, categories, primary_category, published_date, updated_at, url)\n",
    "          VALUES (id, title, abstract, authors, categories, primary_category, S.published_date, S.updated_at, url)\n",
    "        \"\"\"\n",
    "        bq_client.query(merge_query).result()\n",
    "        print(\"Merge complete.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"BigQuery Error: {e}\")\n",
    "        # Если ошибка валидации, выводим подробности\n",
    "        if hasattr(job, 'errors') and job.errors:\n",
    "            print(\"Detailed errors:\", job.errors)\n",
    "else:\n",
    "    print(\"No new articles found.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7861940b-080c-4a4e-b043-35538e2d9bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 3. TRANSFORM (SQL Analytics)\n",
    "# ==========================================\n",
    "\n",
    "print(\"Running SQL analytics...\")\n",
    "\n",
    "# А. Расчет весов связей (CROSS-CATEGORY CO-OCCURRENCE)\n",
    "# Анализируем данные за год\n",
    "links_query = f\"\"\"\n",
    "WITH exploded AS (\n",
    "    SELECT id, cat \n",
    "    FROM `{FULL_TABLE_REF}`, UNNEST(categories) cat\n",
    "    WHERE published_date >= DATE_SUB(CURRENT_DATE(), INTERVAL {ANALYSIS_PERIOD_DAYS} DAY)\n",
    ")\n",
    "SELECT \n",
    "    t1.cat as source, \n",
    "    t2.cat as target, \n",
    "    COUNT(*) as weight\n",
    "FROM exploded t1\n",
    "JOIN exploded t2 ON t1.id = t2.id\n",
    "WHERE t1.cat < t2.cat -- Избегаем дублей (A-B и B-A) и петель (A-A)\n",
    "GROUP BY 1, 2\n",
    "HAVING weight > 2 -- Фильтр шума\n",
    "ORDER BY weight DESC\n",
    "\"\"\"\n",
    "links_df = bq_client.query(links_query).to_dataframe()\n",
    "\n",
    "# Б. Выбор ТОП статей для узлов\n",
    "# Ранжирование: (Кол-во категорий * 100) + (Дней до сегодня / 10)\n",
    "# Смысл: Мультидисциплинарные статьи всплывают вверх, свежие тоже.\n",
    "nodes_query = f\"\"\"\n",
    "WITH scored_articles AS (\n",
    "    SELECT \n",
    "        *,\n",
    "        (ARRAY_LENGTH(categories) * 100) AS score\n",
    "    FROM `{FULL_TABLE_REF}`\n",
    "    WHERE published_date >= DATE_SUB(CURRENT_DATE(), INTERVAL {ANALYSIS_PERIOD_DAYS} DAY)\n",
    "),\n",
    "ranked AS (\n",
    "    SELECT \n",
    "        *,\n",
    "        ROW_NUMBER() OVER(PARTITION BY primary_category ORDER BY score DESC, published_date DESC) as rank\n",
    "    FROM scored_articles\n",
    ")\n",
    "SELECT * FROM ranked WHERE rank <= {TOP_LIMIT_PER_CAT}\n",
    "\"\"\"\n",
    "nodes_df = bq_client.query(nodes_query).to_dataframe()\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 4. JSON GENERATION (Graph Construction)\n",
    "# ==========================================\n",
    "\n",
    "final_nodes = []\n",
    "final_links = []\n",
    "seen_nodes = set()\n",
    "\n",
    "# 4.1. Узлы Дисциплин\n",
    "for code, info in TAXONOMY.items():\n",
    "    final_nodes.append({\n",
    "        \"id\": code,\n",
    "        \"label\": info['name'],\n",
    "        \"type\": \"discipline\",\n",
    "        \"description\": info['description'],\n",
    "        \"cluster\": code,\n",
    "        \"val\": 30\n",
    "    })\n",
    "    seen_nodes.add(code)\n",
    "\n",
    "# 4.2. Узлы Статей\n",
    "for _, row in nodes_df.iterrows():\n",
    "    # Массивы приходят как ndarray, конвертируем в list\n",
    "    cats = list(row['categories'])\n",
    "    auths = list(row['authors'])\n",
    "    \n",
    "    if row['id'] not in seen_nodes:\n",
    "        final_nodes.append({\n",
    "            \"id\": row['id'],\n",
    "            \"label\": row['title'],\n",
    "            \"type\": \"article\",\n",
    "            \"description\": row['abstract'],\n",
    "            \"authors\": auths,\n",
    "            \"cluster\": row['primary_category'],\n",
    "            \"val\": 5,\n",
    "            \"url\": row['url']\n",
    "        })\n",
    "        seen_nodes.add(row['id'])\n",
    "    \n",
    "    # Связи CONTAINS (Дисциплина -> Статья)\n",
    "    for c in cats:\n",
    "        if c in TAXONOMY:\n",
    "            final_links.append({\n",
    "                \"source\": c,\n",
    "                \"target\": row['id'],\n",
    "                \"type\": \"CONTAINS\",\n",
    "                \"val\": 1\n",
    "            })\n",
    "\n",
    "# 4.3. Связи RELATED (Дисциплина <-> Дисциплина) из SQL\n",
    "max_w = links_df['weight'].max() if not links_df.empty else 1\n",
    "for _, row in links_df.iterrows():\n",
    "    if row['source'] in TAXONOMY and row['target'] in TAXONOMY:\n",
    "        final_links.append({\n",
    "            \"source\": row['source'],\n",
    "            \"target\": row['target'],\n",
    "            \"type\": \"RELATED\",\n",
    "            \"label\": f\"{row['weight']} shared articles\",\n",
    "            \"val\": (row['weight'] / max_w) * 10\n",
    "        })\n",
    "\n",
    "# 4.4. Связи DEPENDS (Статья <-> Статья) - оставляем в Python, \n",
    "# т.к. SQL для пересечения массивов строк сложен и дорог\n",
    "# Работаем только с выбранными \"топ\" статьями\n",
    "articles_list = [n for n in final_nodes if n['type'] == 'article']\n",
    "from itertools import combinations\n",
    "\n",
    "for a1, a2 in combinations(articles_list, 2):\n",
    "    s1 = set(a1['authors'])\n",
    "    s2 = set(a2['authors'])\n",
    "    common = list(s1.intersection(s2))\n",
    "    if common:\n",
    "        final_links.append({\n",
    "            \"source\": a1['id'],\n",
    "            \"target\": a2['id'],\n",
    "            \"type\": \"DEPENDS\",\n",
    "            \"label\": \"Authors: \" + \", \".join(common),\n",
    "            \"val\": len(common) * 2\n",
    "        })\n",
    "\n",
    "# Сохранение\n",
    "output_data = {\"nodes\": final_nodes, \"links\": final_links}\n",
    "with open(\"graph_data.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(output_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Done. Nodes: {len(final_nodes)}, Links: {len(final_links)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477e3601-c43d-4409-bc89-b18454a24f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting historical backfill from 2025-1 to 2025-12...\n",
      "\n",
      "Processing: 2025-01 (Query: submittedDate:[202501010000 TO 202501312359])\n",
      "   Found 3668 articles.\n",
      "   Uploaded 3668 articles to BigQuery.\n",
      "   Sleeping 2 seconds...\n",
      "\n",
      "Processing: 2025-02 (Query: submittedDate:[202502010000 TO 202502282359])\n",
      "   Found 3667 articles.\n",
      "   Uploaded 3667 articles to BigQuery.\n",
      "   Sleeping 2 seconds...\n",
      "\n",
      "Processing: 2025-03 (Query: submittedDate:[202503010000 TO 202503312359])\n",
      "   Found 4185 articles.\n",
      "   Uploaded 4185 articles to BigQuery.\n",
      "   Sleeping 2 seconds...\n",
      "\n",
      "Processing: 2025-04 (Query: submittedDate:[202504010000 TO 202504302359])\n",
      "   Found 4009 articles.\n",
      "   Uploaded 4009 articles to BigQuery.\n",
      "   Sleeping 2 seconds...\n",
      "\n",
      "Processing: 2025-05 (Query: submittedDate:[202505010000 TO 202505312359])\n",
      "   Found 4075 articles.\n",
      "   Uploaded 4075 articles to BigQuery.\n",
      "   Sleeping 2 seconds...\n",
      "\n",
      "Processing: 2025-06 (Query: submittedDate:[202506010000 TO 202506302359])\n"
     ]
    }
   ],
   "source": [
    "#####  Код для прокачки истории статей за год (2025)\n",
    "import calendar\n",
    "from datetime import date, timedelta\n",
    "import time\n",
    "\n",
    "# ==========================================\n",
    "# СПЕЦИАЛЬНЫЙ СКРИПТ: ИСТОРИЧЕСКАЯ ЗАГРУЗКА (BACKFILL)\n",
    "# ==========================================\n",
    "\n",
    "# Настраиваем период загрузки (весь 2025 год)\n",
    "START_YEAR = 2025\n",
    "START_MONTH = 1\n",
    "END_YEAR = 2025\n",
    "END_MONTH = 12 \n",
    "\n",
    "print(f\"Starting historical backfill from {START_YEAR}-{START_MONTH} to {END_YEAR}-{END_MONTH}...\")\n",
    "\n",
    "# Функция для генерации списка (год, месяц)\n",
    "def get_month_range(start_year, start_month, end_year, end_month):\n",
    "    current_year, current_month = start_year, start_month\n",
    "    while (current_year < end_year) or (current_year == end_year and current_month <= end_month):\n",
    "        yield current_year, current_month\n",
    "        # Переход к следующему месяцу\n",
    "        if current_month == 12:\n",
    "            current_month = 1\n",
    "            current_year += 1\n",
    "        else:\n",
    "            current_month += 1\n",
    "\n",
    "# Проходим цикл по каждому месяцу\n",
    "for year, month in get_month_range(START_YEAR, START_MONTH, END_YEAR, END_MONTH):\n",
    "    \n",
    "    # 1. Определяем даты начала и конца месяца\n",
    "    # last_day возвращает кол-во дней в месяце (28, 30, 31)\n",
    "    _, last_day = calendar.monthrange(year, month)\n",
    "    \n",
    "    # Формат дат для arXiv API: YYYYMMDDHHMM\n",
    "    # submittedDate:[202501010000 TO 202501312359]\n",
    "    date_query = f\"submittedDate:[{year}{month:02d}010000 TO {year}{month:02d}{last_day}2359]\"\n",
    "    \n",
    "    print(f\"\\nProcessing: {year}-{month:02d} (Query: {date_query})\")\n",
    "    \n",
    "    # 2. Формируем запрос\n",
    "    search = arxiv.Search(\n",
    "        query = f\"cat:math.* AND {date_query}\",\n",
    "        max_results = 10000, # С запасом на месяц (обычно в Math выходит 3-4к статей в месяц)\n",
    "        sort_by = arxiv.SortCriterion.SubmittedDate\n",
    "    )\n",
    "    \n",
    "    batch_rows = []\n",
    "    \n",
    "    # 3. Скачиваем статьи\n",
    "    try:\n",
    "        results = list(arxiv_client.results(search))\n",
    "        print(f\"   Found {len(results)} articles.\")\n",
    "        \n",
    "        for r in results:\n",
    "            # Фильтрация категорий\n",
    "            cats = [c for c in r.categories if c in TAXONOMY]\n",
    "            if not cats: continue\n",
    "\n",
    "            batch_rows.append({\n",
    "                \"id\": r.entry_id.split('/')[-1],\n",
    "                \"title\": r.title.replace('\\n', ' '),\n",
    "                \"abstract\": r.summary.replace('\\n', ' '),\n",
    "                \"authors\": [a.name for a in r.authors],\n",
    "                \"categories\": cats,\n",
    "                \"primary_category\": r.primary_category,\n",
    "                \"published_date\": r.published.strftime(\"%Y-%m-%d\"),\n",
    "                \"updated_at\": datetime.now().isoformat(),\n",
    "                \"url\": r.entry_id\n",
    "            })\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   Error fetching from arXiv: {e}\")\n",
    "        continue\n",
    "\n",
    "    # 4. Загружаем в BigQuery (если есть что грузить)\n",
    "    if batch_rows:\n",
    "        try:\n",
    "            temp_table_id = f\"{PROJECT_ID}.{DATASET_ID}.temp_upload\"\n",
    "            \n",
    "            # Та самая ПОЛНАЯ схема, чтобы не было ошибки 400\n",
    "            job_config = bigquery.LoadJobConfig(\n",
    "                schema=[\n",
    "                    bigquery.SchemaField(\"id\", \"STRING\"),\n",
    "                    bigquery.SchemaField(\"title\", \"STRING\"),\n",
    "                    bigquery.SchemaField(\"abstract\", \"STRING\"),\n",
    "                    bigquery.SchemaField(\"authors\", \"STRING\", mode=\"REPEATED\"),\n",
    "                    bigquery.SchemaField(\"categories\", \"STRING\", mode=\"REPEATED\"),\n",
    "                    bigquery.SchemaField(\"primary_category\", \"STRING\"),\n",
    "                    bigquery.SchemaField(\"published_date\", \"DATE\"),\n",
    "                    bigquery.SchemaField(\"updated_at\", \"TIMESTAMP\"),\n",
    "                    bigquery.SchemaField(\"url\", \"STRING\"),\n",
    "                ],\n",
    "                write_disposition=\"WRITE_TRUNCATE\",\n",
    "                source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON\n",
    "            )\n",
    "            \n",
    "            # Заливаем во временную таблицу\n",
    "            load_job = bq_client.load_table_from_json(batch_rows, temp_table_id, job_config=job_config)\n",
    "            load_job.result()\n",
    "            \n",
    "            # Делаем MERGE в основную таблицу\n",
    "            merge_query = f\"\"\"\n",
    "            MERGE `{FULL_TABLE_REF}` T\n",
    "            USING `{temp_table_id}` S\n",
    "            ON T.id = S.id\n",
    "            WHEN MATCHED THEN\n",
    "              UPDATE SET \n",
    "                T.title = S.title,\n",
    "                T.categories = S.categories,\n",
    "                T.updated_at = S.updated_at\n",
    "            WHEN NOT MATCHED THEN\n",
    "              INSERT (id, title, abstract, authors, categories, primary_category, published_date, updated_at, url)\n",
    "              VALUES (id, title, abstract, authors, categories, primary_category, S.published_date, S.updated_at, url)\n",
    "            \"\"\"\n",
    "            bq_client.query(merge_query).result()\n",
    "            print(f\"   Uploaded {len(batch_rows)} articles to BigQuery.\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   BigQuery Error: {e}\")\n",
    "            if 'load_job' in locals() and load_job.errors:\n",
    "                print(load_job.errors)\n",
    "    else:\n",
    "        print(\"   No articles to upload.\")\n",
    "\n",
    "    # 5. Пауза, чтобы arXiv не забанил IP\n",
    "    print(\"   Sleeping 2 seconds...\")\n",
    "    time.sleep(2)\n",
    "\n",
    "print(\"\\nFull backfill complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47637147-98ef-4c59-baad-5e0bcc038592",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
